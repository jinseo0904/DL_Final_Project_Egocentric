{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torchvision.utils as utils\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CharadesEgo_v1_test.csv', 'CharadesEgo_v1_test_only1st.csv', 'CharadesEgo_v1_test_only3rd.csv', 'CharadesEgo_v1_train.csv', 'CharadesEgo_v1_train_only1st.csv', 'CharadesEgo_v1_train_only3rd.csv', 'Charades_v1_classes.txt', 'Charades_v1_classify.m', 'Charades_v1_localize.m', 'Charades_v1_mapping.txt', 'Charades_v1_objectclasses.txt', 'Charades_v1_verbclasses.txt', 'license.txt', 'README.txt']\n"
     ]
    }
   ],
   "source": [
    "annotation_path = '/mnt/d/CharadesEgo/Annotations'\n",
    "# print all files in the annotation path\n",
    "print(os.listdir(annotation_path))\n",
    "# train_csv\n",
    "train_csv = pd.read_csv('/mnt/d/CharadesEgo/Annotations/CharadesEgo_v1_train_only1st.csv')\n",
    "test_csv = pd.read_csv('/mnt/d/CharadesEgo/Annotations/CharadesEgo_v1_test_only1st.csv')\n",
    "VIDEO_PATH = '/mnt/d/CharadesEgo/CharadesEgo_v1_480'\n",
    "CLASS_MAP = '/mnt/d/CharadesEgo/Annotations/Charades_v1_classes.txt'\n",
    "# # print train_csv\n",
    "# print(train_csv.head())\n",
    "# # print test_csv\n",
    "# print(test_csv.head())\n",
    "\n",
    "# make sure the full content of each cell is visible\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 classes:\n",
      "c000: Holding some clothes\n",
      "c001: Putting clothes somewhere\n",
      "c002: Taking some clothes from somewhere\n",
      "c003: Throwing clothes somewhere\n",
      "c004: Tidying some clothes\n",
      "c005: Washing some clothes\n",
      "c006: Closing a door\n",
      "c007: Fixing a door\n",
      "c008: Opening a door\n",
      "c009: Putting something on a table\n"
     ]
    }
   ],
   "source": [
    "# read CLASS_MAP into a dictionary mapping class IDs to descriptions\n",
    "class_map = {}\n",
    "with open(CLASS_MAP, 'r') as f:\n",
    "    for line in f:\n",
    "        class_id, description = line.strip().split(' ', 1)\n",
    "        class_map[class_id] = description\n",
    "print(\"First 10 classes:\")\n",
    "for i, (class_id, desc) in enumerate(class_map.items()):\n",
    "    if i >= 10:\n",
    "        break\n",
    "    print(f\"{class_id}: {desc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_action_segments(action_segments):\n",
    "    segments = []\n",
    "    for segment in action_segments:\n",
    "        # split segment by space\n",
    "        segment = segment.split(' ')\n",
    "        # convert to tuple of str, float, float\n",
    "        segments.append((segment[0], float(segment[1]), float(segment[2])))\n",
    "    return segments\n",
    "\n",
    "# apply parse_action_segments to actions_list column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3084\n",
      "3078\n",
      "0                    [(c156, 3.9, 12.0), (c061, 8.2, 12.5), (c106, 9.9, 18.4), (c067, 12.8, 17.9), (c008, 12.2, 18.7), (c065, 11.2, 18.1), (c104, 11.7, 17.6), (c006, 12.5, 17.9), (c156, 3.9, 31.25), (c107, 1.0, 31.25), (c061, 1.3, 29.7), (c106, 10.9, 31.25)]\n",
      "1    [(c109, 19.9, 24.29), (c107, 16.8, 23.8), (c106, 10.1, 21.0), (c077, 0.0, 3.8), (c126, 0.0, 3.6), (c110, 5.4, 10.1), (c154, 0.0, 18.3), (c061, 18.8, 24.29), (c079, 6.2, 10.8), (c110, 7.6, 24.29), (c080, 5.0, 10.4), (c076, 5.1, 10.8), (c092, 8.0, 24.29)]\n",
      "2                                                                           [(c107, 16.5, 22.04), (c110, 15.3, 21.0), (c109, 23.1, 22.04), (c017, 13.5, 21.4), (c015, 0.0, 16.6), (c153, 10.2, 16.3), (c059, 0.0, 22.04), (c106, 17.3, 22.04), (c011, 0.0, 22.04)]\n",
      "3                                                                                                       [(c073, 14.9, 20.0), (c095, 0.0, 15.9), (c070, 10.2, 20.0), (c000, 0.0, 20.0), (c002, 0.1, 20.0), (c035, 0.0, 20.0), (c154, 12.3, 17.9), (c123, 0.0, 5.2)]\n",
      "5                                                                                                                         [(c127, 0.7, 13.1), (c030, 10.4, 15.6), (c026, 11.6, 18.6), (c098, 0.0, 14.1), (c102, 1.2, 14.3), (c099, 8.6, 13.7), (c115, 10.7, 15.9)]\n",
      "Name: actions_list, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# compare the number of rows before and after\n",
    "print(len(train_csv))\n",
    "\n",
    "# from train_csv, keep only rows where actions is str\n",
    "train_csv = train_csv[train_csv['actions'].apply(type) == str]\n",
    "# print the number of rows after filtering\n",
    "print(len(train_csv))\n",
    "\n",
    "# convert actions column to list\n",
    "train_csv['actions_list'] = train_csv['actions'].apply(lambda x: x.split(';'))\n",
    "\n",
    "# apply parse_action_segments to actions_list column\n",
    "train_csv['actions_list'] = train_csv['actions_list'].apply(parse_action_segments)\n",
    "print(train_csv['actions_list'].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34256\n",
      "  action  start   end  video_id  action_duration\n",
      "0   c156    3.9  12.0  D3TR8EGO              8.1\n",
      "1   c061    8.2  12.5  D3TR8EGO              4.3\n",
      "2   c106    9.9  18.4  D3TR8EGO              8.5\n",
      "3   c067   12.8  17.9  D3TR8EGO              5.1\n",
      "4   c008   12.2  18.7  D3TR8EGO              6.5\n"
     ]
    }
   ],
   "source": [
    "# merge all action_list values into a single list with corresponding video IDs\n",
    "all_action_segments = [(segment, id) for id, action_list in zip(train_csv['id'], train_csv['actions_list']) for segment in action_list]\n",
    "print(len(all_action_segments))\n",
    "\n",
    "# convert all_action_segments to a pandas DataFrame\n",
    "action_segments_df = pd.DataFrame([(action, start, end, video_id) for (action, start, end), video_id in all_action_segments], \n",
    "                                columns=['action', 'start', 'end', 'video_id'])\n",
    "\n",
    "# add a column for the video length (start to end)\n",
    "action_segments_df['action_duration'] = action_segments_df['end'] - action_segments_df['start']\n",
    "print(action_segments_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_csv = pd.read_csv('/mnt/d/CharadesEgo/Annotations/CharadesEgo_v1_test_only1st.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean action duration: 11.09 seconds\n",
      "Standard deviation: 8.47 seconds\n",
      "\n",
      "Shapiro-Wilk test results:\n",
      "Statistic: 0.8938\n",
      "p-value: 0.0000e+00\n",
      "\n",
      "Null hypothesis: Distribution is normal\n",
      "The distribution is not normal (α=0.05)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinseo/miniconda3/envs/tf/lib/python3.9/site-packages/scipy/stats/_morestats.py:1882: UserWarning: p-value may not be accurate for N > 5000.\n",
      "  warnings.warn(\"p-value may not be accurate for N > 5000.\")\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# get a histogram of action_duration\n",
    "action_segments_df['action_duration'].hist(bins=100)\n",
    "plt.show()\n",
    "\n",
    "# Perform Shapiro-Wilk test for normality\n",
    "from scipy import stats\n",
    "\n",
    "# Get descriptive statistics\n",
    "mean_duration = action_segments_df['action_duration'].mean()\n",
    "std_duration = action_segments_df['action_duration'].std()\n",
    "print(f\"Mean action duration: {mean_duration:.2f} seconds\")\n",
    "print(f\"Standard deviation: {std_duration:.2f} seconds\")\n",
    "\n",
    "# Perform normality test\n",
    "stat, p_value = stats.shapiro(action_segments_df['action_duration'])\n",
    "print(\"\\nShapiro-Wilk test results:\")\n",
    "print(f\"Statistic: {stat:.4f}\")\n",
    "print(f\"p-value: {p_value:.4e}\")\n",
    "print(\"\\nNull hypothesis: Distribution is normal\")\n",
    "print(f\"The distribution is {'normal' if p_value > 0.05 else 'not normal'} (α=0.05)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c000',\n",
       " 'c001',\n",
       " 'c002',\n",
       " 'c003',\n",
       " 'c004',\n",
       " 'c005',\n",
       " 'c006',\n",
       " 'c007',\n",
       " 'c008',\n",
       " 'c009',\n",
       " 'c010',\n",
       " 'c011',\n",
       " 'c012',\n",
       " 'c013',\n",
       " 'c014',\n",
       " 'c015',\n",
       " 'c016',\n",
       " 'c017',\n",
       " 'c018',\n",
       " 'c019',\n",
       " 'c020',\n",
       " 'c021',\n",
       " 'c022',\n",
       " 'c023',\n",
       " 'c024',\n",
       " 'c025',\n",
       " 'c026',\n",
       " 'c027',\n",
       " 'c028',\n",
       " 'c029',\n",
       " 'c030',\n",
       " 'c031',\n",
       " 'c032',\n",
       " 'c033',\n",
       " 'c034',\n",
       " 'c035',\n",
       " 'c036',\n",
       " 'c037',\n",
       " 'c038',\n",
       " 'c039',\n",
       " 'c040',\n",
       " 'c041',\n",
       " 'c042',\n",
       " 'c043',\n",
       " 'c044',\n",
       " 'c045',\n",
       " 'c046',\n",
       " 'c047',\n",
       " 'c048',\n",
       " 'c049',\n",
       " 'c050',\n",
       " 'c051',\n",
       " 'c052',\n",
       " 'c053',\n",
       " 'c054',\n",
       " 'c055',\n",
       " 'c056',\n",
       " 'c057',\n",
       " 'c058',\n",
       " 'c059',\n",
       " 'c060',\n",
       " 'c061',\n",
       " 'c062',\n",
       " 'c063',\n",
       " 'c064',\n",
       " 'c065',\n",
       " 'c066',\n",
       " 'c067',\n",
       " 'c068',\n",
       " 'c069',\n",
       " 'c070',\n",
       " 'c071',\n",
       " 'c072',\n",
       " 'c073',\n",
       " 'c074',\n",
       " 'c075',\n",
       " 'c076',\n",
       " 'c077',\n",
       " 'c078',\n",
       " 'c079',\n",
       " 'c080',\n",
       " 'c081',\n",
       " 'c082',\n",
       " 'c083',\n",
       " 'c084',\n",
       " 'c085',\n",
       " 'c086',\n",
       " 'c087',\n",
       " 'c088',\n",
       " 'c089',\n",
       " 'c090',\n",
       " 'c091',\n",
       " 'c092',\n",
       " 'c093',\n",
       " 'c094',\n",
       " 'c095',\n",
       " 'c096',\n",
       " 'c097',\n",
       " 'c098',\n",
       " 'c099',\n",
       " 'c100',\n",
       " 'c101',\n",
       " 'c102',\n",
       " 'c103',\n",
       " 'c104',\n",
       " 'c105',\n",
       " 'c106',\n",
       " 'c107',\n",
       " 'c108',\n",
       " 'c109',\n",
       " 'c110',\n",
       " 'c111',\n",
       " 'c112',\n",
       " 'c113',\n",
       " 'c114',\n",
       " 'c115',\n",
       " 'c116',\n",
       " 'c117',\n",
       " 'c118',\n",
       " 'c119',\n",
       " 'c120',\n",
       " 'c121',\n",
       " 'c122',\n",
       " 'c123',\n",
       " 'c124',\n",
       " 'c125',\n",
       " 'c126',\n",
       " 'c127',\n",
       " 'c128',\n",
       " 'c129',\n",
       " 'c130',\n",
       " 'c131',\n",
       " 'c132',\n",
       " 'c133',\n",
       " 'c134',\n",
       " 'c135',\n",
       " 'c136',\n",
       " 'c137',\n",
       " 'c138',\n",
       " 'c139',\n",
       " 'c140',\n",
       " 'c141',\n",
       " 'c142',\n",
       " 'c143',\n",
       " 'c144',\n",
       " 'c145',\n",
       " 'c146',\n",
       " 'c147',\n",
       " 'c148',\n",
       " 'c149',\n",
       " 'c150',\n",
       " 'c151',\n",
       " 'c152',\n",
       " 'c153',\n",
       " 'c154',\n",
       " 'c155',\n",
       " 'c156']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(class_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique action classes: 157\n",
      "\n",
      "First few action mappings:\n",
      "c000 -> 0\n",
      "c001 -> 1\n",
      "c002 -> 2\n",
      "c003 -> 3\n",
      "c004 -> 4\n",
      "Training set size: 27404, Validation set size: 6852\n",
      "Training samples: 27404, Validation samples: 6852\n",
      "Number of batches in train_loader: 6851\n",
      "\n",
      "Sample batch labels shape: torch.Size([4])\n",
      "Sample labels: tensor([ 33, 132,   4,  65])\n",
      "Label types: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# First, create a mapping of action classes to integers\n",
    "unique_actions = sorted(list(class_map.keys()))\n",
    "action_to_idx = {action: idx for idx, action in enumerate(unique_actions)}\n",
    "idx_to_action = {idx: action for action, idx in action_to_idx.items()}\n",
    "\n",
    "print(f\"Number of unique action classes: {len(unique_actions)}\")\n",
    "print(\"\\nFirst few action mappings:\")\n",
    "for i in range(min(5, len(unique_actions))):\n",
    "    print(f\"{unique_actions[i]} -> {action_to_idx[unique_actions[i]]}\")\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(action_segments_df, test_size=0.2, random_state=42, stratify=action_segments_df['action'])\n",
    "\n",
    "print(f\"Training set size: {len(train_df)}, Validation set size: {len(val_df)}\")\n",
    "\n",
    "# Create a custom dataset for video segments\n",
    "class VideoSegmentDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, segments_df, video_path, num_frames=16, frame_size=(160, 160), action_to_idx=None):\n",
    "        self.segments_df = segments_df\n",
    "        self.video_path = video_path\n",
    "        self.num_frames = num_frames\n",
    "        self.frame_size = frame_size\n",
    "        self.action_to_idx = action_to_idx\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize(frame_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.segments_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get video info\n",
    "        video_id = self.segments_df.iloc[idx]['video_id']\n",
    "        start_time = self.segments_df.iloc[idx]['start']\n",
    "        end_time = self.segments_df.iloc[idx]['end']\n",
    "        action_class = self.segments_df.iloc[idx]['action']\n",
    "        \n",
    "        # Convert action class to integer index\n",
    "        if self.action_to_idx is not None:\n",
    "            action_idx = self.action_to_idx[action_class]\n",
    "        else:\n",
    "            action_idx = action_class  # If no mapping provided, assume it's already an integer\n",
    "        \n",
    "        # Load video\n",
    "        video_file = os.path.join(self.video_path, video_id + '.mp4')\n",
    "        cap = cv2.VideoCapture(video_file)\n",
    "        \n",
    "        # Get video properties\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "        \n",
    "        # Calculate start and end frames\n",
    "        start_frame = int(start_time * fps)\n",
    "        end_frame = min(int(end_time * fps), total_frames)\n",
    "        \n",
    "        # Sample frames evenly from the segment\n",
    "        if end_frame - start_frame >= self.num_frames:\n",
    "            # If we have more frames than needed, sample evenly\n",
    "            frame_indices = np.linspace(start_frame, end_frame-1, self.num_frames, dtype=int)\n",
    "        else:\n",
    "            # If we have fewer frames, loop the video\n",
    "            frame_indices = np.array([(start_frame + i) % (end_frame - start_frame) + start_frame \n",
    "                                     for i in range(self.num_frames)])\n",
    "        \n",
    "        # Extract frames\n",
    "        frames = []\n",
    "        for frame_idx in frame_indices:\n",
    "            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                # If frame reading fails, create a blank frame\n",
    "                frame = np.zeros((self.frame_size[0], self.frame_size[1], 3), dtype=np.uint8)\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        \n",
    "        cap.release()\n",
    "        \n",
    "        # Stack frames to create video tensor [C, T, H, W]\n",
    "        video_tensor = torch.stack(frames, dim=1)  # Shape: [3, 32, 224, 224]\n",
    "        \n",
    "        return video_tensor, action_idx\n",
    "\n",
    "# Use train_df for training dataset\n",
    "train_dataset = VideoSegmentDataset(train_df, VIDEO_PATH, num_frames=8, frame_size=(80, 80), action_to_idx=action_to_idx)\n",
    "\n",
    "# Use val_df for validation dataset with the same action_to_idx mapping\n",
    "val_dataset = VideoSegmentDataset(val_df, VIDEO_PATH, num_frames=8, frame_size=(80, 80), action_to_idx=action_to_idx)\n",
    "\n",
    "# Create dataloaders\n",
    "batch_size = 4  # Reduced from 8\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    num_workers=2,  # Reduced from 4\n",
    "    pin_memory=True\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    num_workers=2,  # Reduced from 4\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")\n",
    "\n",
    "# Verify that the labels are now integers\n",
    "sample_batch = next(iter(train_loader))\n",
    "videos, labels = sample_batch\n",
    "print(f\"\\nSample batch labels shape: {labels.shape}\")\n",
    "print(f\"Sample labels: {labels}\")\n",
    "print(f\"Label types: {labels.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# check pytorch version\n",
    "print(torch.__version__)\n",
    "\n",
    "# check cuda version\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct a simple 3D CNN model for video classification\n",
    "class VideoCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VideoCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv3d(3, 64, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool1 = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2))\n",
    "        self.conv2 = nn.Conv3d(64, 128, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool2 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        self.conv3 = nn.Conv3d(128, 256, kernel_size=(3, 3, 3), padding=(1, 1, 1))\n",
    "        self.pool3 = nn.MaxPool3d(kernel_size=(2, 2, 2), stride=(2, 2, 2))\n",
    "        # Calculate the correct input size for the first fully connected layer\n",
    "        # For 16 frames of 160x160 input, after pooling operations\n",
    "        self.fc1 = nn.Linear(51200, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 157)  # 157 classes (c000 to c156)\n",
    "        \n",
    "        # Add dropout for regularization\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.pool3(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.dropout(x)  # Add dropout\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)  # Add dropout\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, criterion, optimizer, num_epochs):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {device}\")\n",
    "        self.to(device)\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Clear GPU cache at the start of each epoch\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            # Initialize metrics for each epoch\n",
    "            running_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            for i, (inputs, labels) in enumerate(train_loader, 0):\n",
    "                # Clear GPU cache periodically (every 10 batches)\n",
    "                if i % 10 == 0:\n",
    "                    torch.cuda.empty_cache()\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                outputs = self(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                running_loss += loss.item()\n",
    "                _, predicted = outputs.max(1)\n",
    "                total += labels.size(0)\n",
    "                correct += predicted.eq(labels).sum().item()\n",
    "                print(f\"Batch {i+1}/{len(train_loader)}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "                # Clear memory after each batch\n",
    "                del outputs, loss\n",
    "                torch.cuda.empty_cache()\n",
    "                \n",
    "            epoch_loss = running_loss / len(train_loader)\n",
    "            epoch_acc = 100. * correct / total\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\")\n",
    "            \n",
    "            # validation\n",
    "            self.eval()\n",
    "            val_loss = 0.0\n",
    "            correct = 0\n",
    "            total = 0\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for inputs, labels in val_loader:\n",
    "                    inputs = inputs.to(device)\n",
    "                    labels = labels.to(device)\n",
    "                    \n",
    "                    outputs = self(inputs)  \n",
    "                    loss = criterion(outputs, labels)\n",
    "                    val_loss += loss.item()\n",
    "                    _, predicted = outputs.max(1)\n",
    "                    total += labels.size(0)\n",
    "                    correct += predicted.eq(labels).sum().item()\n",
    "                    \n",
    "            val_loss = val_loss / len(val_loader)\n",
    "            val_acc = 100. * correct / total\n",
    "            print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.2f}%\")\n",
    "            \n",
    "        print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'VideoCNN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# initialize model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mVideoCNN\u001b[49m()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define hyperparameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'VideoCNN' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model = VideoCNN()\n",
    "\n",
    "# Define hyperparameters\n",
    "num_epochs = 1\n",
    "learning_rate = 0.001\n",
    "batch_size = 6\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "model.train_model(train_loader, val_loader, criterion, optimizer, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add this code to check GPU memory usage\n",
    "def print_gpu_memory():\n",
    "    print(f\"GPU Memory Usage:\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
    "    print(f\"Cached: {torch.cuda.memory_reserved() / 1024**2:.2f} MB\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
